{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders 3 -- Sequence Recommenders (45m) \n",
    "\n",
    "## Goals of this practical:\n",
    "\n",
    "- Understand the sequence recommendation framework (~5min)\n",
    "- Load/Format dataset (~5min)\n",
    "- Understand/train the prod2vec model (~10min)\n",
    "- Evaluate (~10min)\n",
    "- Visualize (~10min)\n",
    "- Fiddle (~5min)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install gensim --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Recommenders:\n",
    "\n",
    "> What will you click next ?\n",
    "\n",
    "The sequence recommendation setting is a particular case of the implicit collaborative filtering setting. Given a sequence of items $i_0,i_1,...,i_n$ the goal is to predict the $i_{(n+1)},...$ items the user will consume. Playlist continuation is a neat use case of sequence recommenders. You've been listening to those songs, what can you listen to now ?\n",
    "\n",
    "\n",
    "This setting differs from the classical collaborative filtering because the history is the recent trace and not the full saved interactions. Also, it's possible to do sequence recommendation without any specific latent user profile. \n",
    "\n",
    "####Â Here we propose to explore this unpersonalized sequence recommandation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used : [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "Here we'll use the same data as before but instead of seeing $(user,item,rating)$ triplets or a $(user,item)$ interaction , we'll see item sequences: $user: [item, item,...]$\n",
    "\n",
    "## Loading Data (same as before but in chronological order):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"dataset/ratings.csv\")\n",
    "ratings = ratings.sort_values(\"timestamp\",ascending=True)\n",
    "print(ratings.iloc[0][\"timestamp\"] < ratings.iloc[-1][\"timestamp\"] ) # just checking \n",
    "\n",
    "ratings.head(5)\n",
    "\n",
    "# we also load titles and create an id2title dictionnary\n",
    "titleCSV = pd.read_csv(\"dataset/movies.csv\")\n",
    "titleCSV.head(5)\n",
    "id2title = titleCSV[[\"movieId\",\"title\"]].set_index(\"movieId\").to_dict()[\"title\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Create sequence datasets:\n",
    "For this task, we need sequences of items as data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Todo): extract all movie sequences (in chronological order) from the dataset:\n",
    "\n",
    "\n",
    "In this dataset, each user has seen at least 20 movies.\n",
    "\n",
    "\n",
    "- We need to extract all movie rating sequences (there is one per user) from the dataset:\n",
    "\n",
    "`sequence_of_movies = [[movieid,...],[movieid,...],...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_of_movies = # To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Todo): Create a train/test dataset\n",
    "\n",
    "Here, we propose as task to predict the last 5 items of each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq,test_seq = [],[]\n",
    "\n",
    "for seq in sequences_of_movies:\n",
    "    train_seq.append(# To Complete)\n",
    "    test_seq.append(# To Complete)\n",
    "    \n",
    "last_consumed_item = [seq[-1] for seq in train_seq] # We save the last consumed item for each list\n",
    "                                                    # We'll use it as a starting point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Todo): Create the list of the most popular movies\n",
    "\n",
    "- Here, popular is the number of times the movie appears in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "counts = # To complete \n",
    "most_popular = # To complete (all movies, sorted by occurences from high->low) \n",
    "num_items = len(most_popular)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "#Most popular looks like this:\n",
    "[356,318,296,2571,593,260,480,110,589,...]\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec skip-gram <=> Prod2Vec\n",
    "\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "The MAIN idea of word2vec is to maximise the similarity (dot product) between the vectors for words which appear close together (in the context of each other) in text, and minimise the similarity of words that do not. \n",
    "\n",
    "This can be applied to products instead of words: it clusters similar products together.\n",
    "\n",
    "\n",
    "#### Paper Abstract:\n",
    "> In recent years online advertising has become increasingly ubiquitous and effective. Advertisements shown to visitors fund sites and apps that publish digital content, manage social networks, and operate e-mail services. Given such large variety of internet resources, determining an appropriate type of advertising for a given platform has become critical to financial success. Native advertisements, namely ads that are similar in look and feel to content, have had great success in news and social feeds. However, to date there has not been a winning formula for ads in e-mail clients. In this paper we describe a system that leverages user purchase history determined from e-mail receipts to deliver highly personalized product ads to Yahoo Mail users. We propose to use a novel neural language-based algorithm specifically tailored for delivering effective product recommendations, which was evaluated against baselines that included showing popular products and products predicted based on co-occurrence. We conducted rigorous offline testing using a large-scale product purchase data set, covering purchases of more than 29 million users from 172 e-commerce websites. Ads in the form of product recommendations were successfully tested on online traffic, where we observed a steady 9% lift in click-through rates over other ad formats in mail, as well as comparable lift in conversion rates. Following successful tests, the system was launched into production during the holiday season of 2014\n",
    "\n",
    "[Prod2Vec Model](https://arxiv.org/abs/1606.07154)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim has the best python implementation of word2vec's algorithms:\n",
    "\n",
    "We can just use these raw implementations. The only thing to do is to consider items as words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "train_seq_str = [list(map(str,seq)) for seq in train_seq] # we just say that our items id's are strings..\n",
    "    \n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=train_seq_str,\n",
    "                                size=50, window=10,               ### here we train a cbow model \n",
    "                                min_count=0,                      \n",
    "                                sample=0.001, ns_exponent=0.75, workers=10,\n",
    "                                sg=1, hs=0, negative=15,          ### set sg to 1 to train a sg model => Prod2Vec\n",
    "                                cbow_mean=0,\n",
    "                                iter=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(w2v.wv.vectors[0])              # The vector of index 0\n",
    "print(w2v.wv.index2word[0])           # codes for the movieId 356\n",
    "print(w2v.wv.vocab[\"356\"].index)      # Inverse mapping\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting similar items:\n",
    "\n",
    "The heart of the algorithm is in the similar item search. As in word2vec, we simply use cosine distance between items to find \"similar items\"\n",
    "\n",
    "### We can search by id's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_ids(w2vmodel,iid,num=5):\n",
    "    \n",
    "    if str(iid) in w2vmodel.wv.vocab:\n",
    "        return [int(iid) for iid,_ in w2vmodel.wv.most_similar(str(iid),topn=num)] \n",
    "    else:\n",
    "        return []\n",
    "\n",
    "get_similar_ids(w2v,last_consumed_item[0],num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Or by vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v.wv[str(last_consumed_item[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_vectors(w2vmodel,vec,num=5):\n",
    "        return [int(iid) for iid,_ in w2vmodel.wv.most_similar(positive=[vec],topn=num)] \n",
    "\n",
    "get_similar_vectors(w2v,w2v.wv[str(last_consumed_item[0])],num=5) # items are strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see if this works\n",
    "\n",
    "We can query by id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "NUM_SIM = 3\n",
    "\n",
    "print(\"Movies similar to: \", id2title[ID])\n",
    "print(\"\")\n",
    "for x in get_similar_ids(w2v,ID,NUM_SIM):\n",
    "    print(\"--> \",id2title[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also query by vector\n",
    "\n",
    "**NOTE:** the 1st results can be the item(s) you've used to query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID = 1\n",
    "NUM_SIM = 4\n",
    "print(\"Movies similar to: \", id2title[ID])\n",
    "print(\"\")\n",
    "for x in get_similar_vectors(w2v,w2v.wv[str(ID)],NUM_SIM):\n",
    "    print(\"--> \",id2title[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our result was the following (for ID = 1 & NUM_SIM = 3)\n",
    "\n",
    ">Movies similar to:  Toy Story (1995)\n",
    ">-  Beauty and the Beast (1991)\n",
    "-   Toy Story 2 (1999)\n",
    "-   Lion King, The (1994)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using vectors enables operations like additions to be made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID1 = 2571\n",
    "ID2 = 589\n",
    "NUM_SIM = 10\n",
    "\n",
    "vec = np.max([w2v.wv[str(ID1)],w2v.wv[str(ID2)]],axis=0)# + w2v.wv[str(318)]\n",
    "\n",
    "print(\"Movies similar to: \", id2title[ID1] , \"+\",  id2title[ID2] )\n",
    "print(\"\")\n",
    "for x in get_similar_vectors(w2v,vec ,NUM_SIM):\n",
    "    print(\"--> \",id2title[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we now have a good base for our sequence recommendation algorithm, let's write something to evaluate our predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Todo) write a `get_relevance_list(proposed_ids,real_ids)` function:\n",
    "\n",
    "This function will be used to compare proposed items w/ real items:\n",
    "\n",
    "\n",
    "- A relevant item is an item which is in the ground truth\n",
    "- It returns a list which length is the number of proposed items filled of 0's and 1's : 0 means the item is not relevant, 1 means it's relevant.\n",
    "\n",
    "- get_relevance_list([1,2,3,4],[1,4,5,6]) should returns [1,0,0,1]  because items 1 and 4 are relevant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevance_list(proposed_ids,real_ids):\n",
    "    real_ids = set(real_ids)\n",
    "    return #To Complete\n",
    "get_relevance_list([1,2,3,4],[1,4,5,6]) #returns [1,0,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test our function on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relevance_list(most_popular[:25],test_seq[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_relevance_list(get_similar_ids(w2v,last_consumed_item[0],25),test_seq[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, now, let's write prediction funtions:\n",
    "\n",
    "- `predict_pop` will recommend the k's most popular items\n",
    "- `predict_w2v` will recommend the k's most similar items to the last one consumed\n",
    "\n",
    "#### (TODO) : complete those functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pop(last_seen,k):\n",
    "    return #To Complete\n",
    "\n",
    "def predict_w2v(last_seen,k):\n",
    "    return #To Complete\n",
    "\n",
    "#data is list of last_consumed:\n",
    "def get_predictions(predict_func,data,truth,k=5):\n",
    "    if k == -1 or k == 0:\n",
    "        k = num_items\n",
    "    return [get_relevance_list(predict_func(last_seen,k),will_see) for last_seen,will_see in zip(data,truth)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The `get_predictions(...)` function returns the relevant list associated to predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following cells should return list of lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_predictions(predict_pop,last_consumed_item[:5],test_seq[:5],3))\n",
    "print(get_predictions(predict_w2v,last_consumed_item[:5],test_seq[:5],3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expected output: \n",
    "```\n",
    "[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "[[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The return of the MRR and nDCG functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [[0,0,1],[0,1,0],[1,0,0],[0,0,0]]\n",
    "\n",
    "def rr(list_items):\n",
    "    relevant_indexes = np.asarray(list_items).nonzero()[0]\n",
    "    \n",
    "    if len(relevant_indexes) > 0:\n",
    "        return 1/(relevant_indexes[0]+1) # arrays are indexed from 0\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def mrr(list_list_items):\n",
    "    return np.mean([rr(list_item) for list_item in list_list_items])\n",
    "\n",
    "mrr(test_list) #0.4583333333333333\n",
    "\n",
    "# The dcg@k is the sum of the relevance, penalized gradually\n",
    "def dcg_at_k(r, k):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        \n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return np.sum(r / np.log2(np.arange(2, r.size + 2)))\n",
    "        \n",
    "    return 0.\n",
    "\n",
    "# test values\n",
    "# r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "# dcg_at_k(r, 1) => 3.0\n",
    "# dcg_at_k(r, 2) => 4.2618595071429155\n",
    "r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "print(dcg_at_k(r, 1))\n",
    "print(dcg_at_k(r, 2))\n",
    "\n",
    "def mean_dcg(rel_lists,k):\n",
    "    return np.mean([dcg_at_k(rel_list,k) for rel_list in rel_lists])\n",
    "\n",
    "# And it's normalized version\n",
    "def ndcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "    \"\"\"\n",
    "    dcg_max =  dcg_at_k(sorted(r)[::-1],k) \n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return dcg_at_k(r, k) / dcg_max\n",
    "\n",
    "# test values\n",
    "# r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "# ndcg_at_k(r, 1) => 1.0\n",
    "# ndcg_at_k(r, 4) => 0.794285\n",
    "    \n",
    "r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]    \n",
    "ndcg_at_k(r, 4)\n",
    "\n",
    "def mean_ndcg(rel_lists,k):\n",
    "    return np.mean([ndcg_at_k(rel_list,k) for rel_list in rel_lists])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see how this naÃ¯ve way of predicting items to show works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_preds = get_predictions(predict_pop,last_consumed_item,test_seq,-1)\n",
    "w2v_preds = get_predictions(predict_w2v,last_consumed_item,test_seq,-1)\n",
    "\n",
    "print(\"1/MRR\")\n",
    "print(1/mrr(pop_preds))\n",
    "print(1/mrr(w2v_preds))\n",
    "print(\"\")\n",
    "print(\"DCG\")\n",
    "print(mean_dcg(pop_preds,5))\n",
    "print(mean_dcg(w2v_preds,5))\n",
    "print(\"\")\n",
    "print(\"nDCG\")\n",
    "print(mean_ndcg(pop_preds,5))\n",
    "print(mean_ndcg(w2v_preds,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Can we do better ?\n",
    "\n",
    "Now, try a different strategy: \n",
    "\n",
    "\n",
    "- History should be discarded from prediction\n",
    "- Instead of basing the prediction on the last seen item, we'll take all the `seen[-n:]` ones (horizon) into account\n",
    "- To aggregate all items, we'll simply take the min rank to take into account the history offset.\n",
    "- Equal scores can be handled using the history offset.\n",
    "Example: \n",
    "\n",
    "> Let's say you chose to use the two last seen items `[item 44, Item 398]` to predict the following items\n",
    "\n",
    "Therefore, using `get_similar_ids` method on both items will yield two lists of **similar** ranked item id's:\n",
    " - Similar to item 44: `[item 1, item 33, item 5]`\n",
    " - Similar to item 398: `[item 25, item 1, item 5]`\n",
    " scores (rank,offset):\n",
    " ```\n",
    " scores := {item 1: (0,0), item 33: (1,0), item 5: (2,0) , item 25: (0,1)}```\n",
    " \n",
    " \n",
    "Then, aggregation by best rank should yield: `[1,25,33,5]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_max_w2v(seen,k,horizon=2):\n",
    "    \n",
    "    # To complete\n",
    "    \n",
    "    return # a list\n",
    "\n",
    "\n",
    "w2v_best_preds = get_predictions(predict_max_w2v,train_seq,test_seq,-1)\n",
    "\n",
    "print(1/mrr(w2v_max_preds))\n",
    "print(mean_dcg(w2v_max_preds,5))\n",
    "print(mean_ndcg(w2v_max_preds,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### => Not really better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize learned embeddings\n",
    "\n",
    "Just like in the 1st practical, we propose to visualize learnt items embeddings with the [Tensorflow projector](https://projector.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function saves embeddings (a numpy array) and associated labels into tsv files.\n",
    "\n",
    "def save_embeddings(embs,dict_label,path=\"saved_word_vectors\"):\n",
    "    \"\"\"\n",
    "    embs is Numpy.array(N,size)\n",
    "    dict_label is {str(word)->int(idx)} or {int(idx)->str(word)}\n",
    "    \"\"\"\n",
    "    def int_first(k,v):\n",
    "        if type(k) == int:\n",
    "            return (k,v)\n",
    "        else:\n",
    "            return (v,k)\n",
    "\n",
    "    np.savetxt(f\"{path}_vectors.tsv\", embs, delimiter=\"\\t\")\n",
    "\n",
    "    #labels \n",
    "    if dict_label:\n",
    "        sorted_labs = np.array([lab for idx,lab in sorted([int_first(k,v) for k,v in dict_label.items()])])\n",
    "        print(sorted_labs)\n",
    "        with open(f\"{path}_metadata.tsv\",\"w\") as metadata_file:\n",
    "            for x in sorted_labs: #hack for space\n",
    "                if len(x.strip()) == 0:\n",
    "                    x = f\"space-{len(x)}\"\n",
    "                    \n",
    "                metadata_file.write(f\"{x}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2title = {i:id2title[int(mid)] for i,mid in enumerate(w2v.wv.index2word)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(w2v.wv.vectors,vec2title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to:\n",
    "\n",
    "- Now, [open this link](https://projector.tensorflow.org/), and select \"load\".\n",
    "- look for saved_word_vectors_vectors.tsv and saved_word_vectors_metadata.tsv. \n",
    "\n",
    "=> These are respectively, the items latent representations and their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters matter when using Word2Vec for Item recommendation:\n",
    "\n",
    "\n",
    "> Skip-gram with negative sampling, a popular variant of Word2vec originally designed and tuned to create word embeddings for Natural Language Processing, has been used to create item embeddings with successful applications in recommendation. While these fields do not share the same type of data, neither evaluate on the same tasks, recommendation applications tend to use the same already tuned hyperparameters values, even if optimal hyperparameters values are often known to be data and task dependent. We thus investigate the marginal importance of each hyperparameter in a recommendation setting through large hyperparameter grid searches on various datasets. Results reveal that optimizing neglected hyperparameters, namely negative sampling distribution, number of epochs, subsampling parameter and window-size, significantly improves performance on a recommendation task, and can increase it by an order of magnitude. Importantly, we find that optimal hyperparameters configurations for Natural Language Processing tasks and Recommendation tasks are noticeably different. \n",
    "\n",
    "[Hyperparameters matter](https://arxiv.org/abs/1804.04212)\n",
    "\n",
    "#### It turns out that  hyperparameters are really important for this task: especially the sampling parameter.  Try and learn multiple models to see how the ns_exponent parameter modifies the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=train_seq_str,\n",
    "                                size=50, window=3,               ### here we train a cbow model \n",
    "                                min_count=0,                      \n",
    "                                sample=0.001, ns_exponent=-0.4, workers=10,\n",
    "                                sg=1, hs=0, negative=15,          ### set sg to 1 to train a sg model => Prod2Vec\n",
    "                                cbow_mean=0,\n",
    "                                iter=25)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_preds = get_predictions(predict_pop,last_consumed_item,test_seq,-1)\n",
    "w2v_preds = get_predictions(predict_w2v,last_consumed_item,test_seq,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1/mrr(pop_preds))\n",
    "print(1/mrr(w2v_preds))\n",
    "\n",
    "print(mean_dcg(pop_preds,5))\n",
    "print(mean_dcg(w2v_preds,5))\n",
    "\n",
    "print(mean_ndcg(pop_preds,5))\n",
    "print(mean_ndcg(w2v_preds,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still got time ? Try making a more clever item selection mechanism:\n",
    "\n",
    "- You could, for example, cluster items in groups (using k-means) and propose the most popular items of the last seen group"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
