{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders 1 -- (1h) \n",
    "\n",
    "## Goals of this practical:\n",
    "\n",
    "1. Have a quick statistical tour of a recommendation dataset (~10min)\n",
    "2. Understand the classical task of rating prediction (~10min)\n",
    "3. Know and implement really simple but tough baselines (~20min)\n",
    "4. Use a scikit like library to build explicit recommender sysems (~20min)\n",
    "\n",
    "\n",
    "## What is collaborative filtering ?\n",
    "\n",
    "On the internet, people rate items (or simply click on them, showing preference). All these ratings can be seen as a matrix coding for all ratings of size (n_user,n_item). It's worth noting that this matrix is highly sparse: No one rates everything, people only rate a subset of items. **Collaborative filtering** is done by leveraging the similarity in people's rating pattern. It enables to find \n",
    "\n",
    "\n",
    "Recommendation (more specifically **collaborative filtering**) goal can be seen as predicting how someone will rate one item. This is akin to filling this matrix, by predicting all missing ratings. The most used algorithms for this task are matrix factorization ones: they take advantage from the fact that a matrix can be decomposed into two sub-matrices: one coding for the users and one coding for the items.\n",
    "\n",
    "## Why predicting ratings and not directly items:\n",
    "\n",
    "Theoritically, the goal of a recommender system is to cherry pick interesting items for one user within a huge collection. Rating prediction is a surrogate problem of item prediction. To get what item to recommend you can, for exemple, simply sort best rated items. It has been shown that improving rating prediction actually improve item prediction.\n",
    "\n",
    "\n",
    "## Data used : [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "In this practical we use a small dataset of user ratings on movies. Specifically, we treat the dataset as list of $(user,item,rating)$ triplets.\n",
    "\n",
    "\n",
    "\n",
    "## Notes: \n",
    "\n",
    "We'll be using pandas for data manipulation but it's not needed to know any special tricks. If you have any pandas related questions, just ask !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites:\n",
    "\n",
    "First, we install and load some packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this to install required packages if needed (and restart kernel !)\n",
    "#! pip install --upgrade pandas\n",
    "#! pip install --upgrade seaborn\n",
    "#! pip install --upgrade scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # we'll use pandas for data analysis\n",
    "import seaborn as sns # we'll use seaborn for nice plots\n",
    "import numpy as np # we'll use numpy for data also\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Should be 0.25.3\n",
    "print(pd.__version__)\n",
    "\n",
    "#Should be 0.9.0\n",
    "print(sns.__version__)\n",
    "\n",
    "## These are the version the notebook has been built on. It could work on old ones, or it could not :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Let's have a quick statistical tour of the ml-small dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/) it is a subset of a bigger data set of movie ratings.\n",
    "\n",
    "> This dataset (ml-latest-small) describes 5-star rating and free-text tagging activity from MovieLens, a movie recommendation service. It contains 100836 ratings and 3683 tag applications across 9742 movies. These data were created by 610 users between March 29, 1996 and September 24, 2018. This dataset was generated on September 26, 2018.\n",
    "\n",
    "> Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's load the data with pandas and see how it looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"dataset/ratings.csv\")\n",
    "ratings.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (WARMUP) Some simple questions we'll try to answer:\n",
    "- How many users are there ?\n",
    "- How many items are there ?\n",
    "- How are ratings distributed ? Per users ? Per items ?\n",
    "- What is the mean rating ?\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Find how many user and item there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings[\"userId\"].values #numpy array\n",
    "ratings[\"movieId\"] #pandas serie\n",
    "#there are 610 users and 9724 items\n",
    "\n",
    "num_users  = # To complete\n",
    "num_items  = # To complete\n",
    "\n",
    "print(f\"there are {num_users} users and {num_items} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Find how sparse the rating matrix is.\n",
    "\n",
    "Rating data is often though of a user/item matrix. Since each user haven't rated every items this matrix is HIGHLY SPARSE (it's mainly filled with zeros). \n",
    "- Find the sparsity of the matrix (spoiler: it's ~1.69% full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rating matrix is only 1.6999683055613624% full\n",
    "\n",
    "sparsity = # To complete\n",
    "print(f\"Rating matrix is only {sparsity}% full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Lets compute some stats:\n",
    "\n",
    "- Find the count/mean/std/min/max of ratings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find/plot the global rating distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find/plot the rating distribution per user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Find/plot the rating distribution per items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stats Takeways:\n",
    "\n",
    "- as it's often the case on rating data, there is a high biais towards good ratings: people tend to rate things they like more than things they dislike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Explicit) Collaborative Filtering  by predicting ratings:\n",
    "\n",
    "The Netflix competition introduced the following predictive framework: the goal is to predict missing ratings using existing ones.\n",
    "\n",
    "In short, the goal is to define a model $f$ to predict how a user $u$ would rate an item $i$ : \n",
    "\n",
    "### $$ f(u,i) = r_{ui} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, let's build a train/test set\n",
    "\n",
    "Here, for simplicity, we take the 1 example out of 5 as a test example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indexes,test_indexes = [],[]\n",
    "\n",
    "for index in range(len(ratings)):\n",
    "    if index%5 == 0:\n",
    "        test_indexes.append(index)\n",
    "    else:\n",
    "        train_indexes.append(index)\n",
    "\n",
    "train_ratings = ratings.iloc[train_indexes].copy()\n",
    "test_ratings = ratings.iloc[test_indexes].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In explicit collaborative filtering, the data are triplets of $(user,item,rating)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how one item is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratings[test_ratings[\"movieId\"] == 131724]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the triplet $(2,131724,5)$, the model goal will be to predict $f(2,131724) = 5$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Recsys **mean** baselines implementations\n",
    "\n",
    "First, before digging into more complicated models, we'll implement three really simple baselines:\n",
    "\n",
    "- The global mean $\\mu$\n",
    "- The user mean $b_u$\n",
    "- The item mean $b_i$\n",
    "\n",
    "Indeed, the rating distribution is not uniform at all (it looks more normal). Therefore, the means are good baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: compute the global training mean, user means and item means\n",
    "\n",
    "- First, compute basic means to use them as baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = train_ratings[\"rating\"].mean()\n",
    "USER_MEANS = train_ratings.groupby(\"userId\")[\"rating\"].mean()\n",
    "ITEM_MEANS = train_ratings.groupby(\"movieId\")[\"rating\"].mean()\n",
    "\n",
    "\n",
    "def mean_rating_pred(user_item):\n",
    "    user = user_item[\"userId\"]\n",
    "    item = user_item[\"movieId\"]\n",
    "    \n",
    "    return ## to complete\n",
    "\n",
    "def user_mean_rating_pred(user_item):\n",
    "    user = user_item[\"userId\"]\n",
    "    item = user_item[\"movieId\"]\n",
    "    \n",
    "    return ## to complete\n",
    "\n",
    "def item_mean_rating_pred(user_item):\n",
    "    user = user_item[\"userId\"]\n",
    "    item = user_item[\"movieId\"]\n",
    "    \n",
    "    return ## to complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should be able to run the following cell to get mean predictions (instead of None's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create the rating prediction columns\n",
    "test_ratings[\"mean_prediction\"] = test_ratings[[\"userId\",\"movieId\"]].apply(mean_rating_pred,axis=1)\n",
    "test_ratings[\"muser_prediction\"] = test_ratings[[\"userId\",\"movieId\"]].apply(user_mean_rating_pred,axis=1) \n",
    "test_ratings[\"mitem_prediction\"] = test_ratings[[\"userId\",\"movieId\"]].apply(item_mean_rating_pred,axis=1) \n",
    "\n",
    "test_ratings.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ok, now that we've got some predictions, let's evaluate them:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics:\n",
    "\n",
    "In explicit collaborative filtering there are three common metrics to compare a ground truth $x$ to a predicted $\\hat{x}$:\n",
    "\n",
    "- Mean Average Error (MAE) : $$\\frac{1}{n}\\sum^n|(x-\\hat{x})|$$\n",
    "- Mean Squared Error (MSE) : $$\\frac{1}{n}\\sum^n(x-\\hat{x})^2$$\n",
    "- Rooted Mean Squared Error (RMSE) : $$\\sqrt{\\frac{1}{n}\\sum^n(x-\\hat{x})^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement those metrics\n",
    "\n",
    "**Note:** The `predictions` and `truth` variables are arrays of predictions/ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def mae(predictions,truth):\n",
    "    return ## To complete\n",
    "\n",
    "def mse(predictions,truth):\n",
    "    return ## To complete\n",
    "\n",
    "def rmse(predictions,truth):\n",
    "    return ## To complete\n",
    "\n",
    "\n",
    "def all_metrics(predictions,truth):\n",
    "    return [f(predictions,truth) for f in [mae,mse,rmse]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You should be able to run the following cell to get the metrics results (Instead of None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"mae\",\"mse\",\"rmse\"]\n",
    "results = pd.DataFrame()\n",
    "\n",
    "results[\"metrics\"] = metrics\n",
    "results[\"mean_prediction\"] = all_metrics(test_ratings[\"mean_prediction\"],test_ratings[\"rating\"])\n",
    "results[\"muser_prediction\"] = all_metrics(test_ratings[\"muser_prediction\"],test_ratings[\"rating\"])\n",
    "results[\"mitem_prediction\"] = all_metrics(test_ratings[\"mitem_prediction\"],test_ratings[\"rating\"])\n",
    "results = results.set_index(\"metrics\")\n",
    "\n",
    "print(results)\n",
    "print(\"\")\n",
    "print('---Best Models / Metrics: ---')\n",
    "results.idxmin(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic explicit collaborative filtering models - using surprise library\n",
    "\n",
    "> Surprise is an easy-to-use Python scikit for recommender systems.\n",
    "\n",
    "> Surprise has a set of built-in algorithms and datasets for you to play with. In its simplest form, it only takes a few lines of code to run a cross-validation procedure:\n",
    "\n",
    "[More details on the official documentation](https://surprise.readthedocs.io/en/stable/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this to install required packages if needed (and restart kernel !)\n",
    "#! pip install --upgrade scikit-surprise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading custom data\n",
    "\n",
    "We want to be able to load our custom dataset in the framework data structures.  \n",
    "To do so, we could either load from a raw file or from a dataframe. We do the latter.\n",
    "\n",
    "(Here is the [Documentation reference](https://surprise.readthedocs.io/en/stable/getting_started.html#use-a-custom-dataset) for loading custom datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NormalPredictor, BaselineOnly, SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "# The columns must correspond to user id, item id and ratings (in that order).\n",
    "data = Dataset.load_from_df(train_ratings[['userId', 'movieId', 'rating']], Reader(rating_scale=(1, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do models work in Surprise ?\n",
    "\n",
    "It's fairly easy, Surprise follows the \"scikit\" way of doing things:\n",
    "```python\n",
    "model = GoodModel(params)\n",
    "model.fit(train_data)\n",
    "predictions = model.predict(test_data)\n",
    "```\n",
    "\n",
    "Ok, let's dig in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The baseline model:\n",
    "\n",
    ">Typical CF data exhibit large user and item effects—systematic tendencies for\n",
    "some users to give higher ratings than others—and for some items to receive\n",
    "higher ratings than others. It is customary to adjust the data by accounting for\n",
    "these effects, which we encapsulate within the baseline estimates. Denote by\n",
    "μ the overall average rating. A baseline estimate for an unknown rating r ui is\n",
    "denoted by b ui and accounts for the user and item effects:\n",
    "\n",
    "## $$ \\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i $$\n",
    "\n",
    "(If user u is unknown, then the bias bu is assumed to be zero. The same applies for item i with bi.)\n",
    "\n",
    "> The parameters b u and b i indicate the observed deviations of user u and item i,\n",
    "respectively, from the average. For example, suppose that we want a baseline\n",
    "estimate for the rating of the movie Titanic by user Joe. Now, say that the\n",
    "average rating over all movies, μ, is 3.7 stars. Furthermore, Titanic is better\n",
    "than an average movie, so it tends to be rated 0.5 stars above the average. On\n",
    "the other hand, Joe is a critical user, who tends to rate 0.3 stars lower than the\n",
    "average. Thus, the baseline estimate for Titanic’s rating by Joe would be 3.9\n",
    "stars by calculating 3.7 − 0.3 + 0.5.\n",
    "\n",
    "(see [Yehuda Koren. Factor in the neighbors: scalable and accurate collaborative filtering. 2010.](http://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/a1-koren.pdf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaselineModel = BaselineOnly()\n",
    "BaselineModel.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Predict\n",
    "If you look at the [Prediction module](https://surprise.readthedocs.io/en/stable/predictions_module.html) documentation,\n",
    "you'll see that it has a little twist w/ respect to the traditional scikit \"predict\". It returns a Prediction \"[namedTuple](https://docs.python.org/3/library/collections.html#collections.namedtuple)\" with the following fields:\n",
    "\n",
    "- uid – The (raw) user id. (raw means it's the one you supplied, not the one automatically computed by surprise)\n",
    "- iid – The (raw) item id.\n",
    "- r_ui (float) – The true rating rui\n",
    "- est (float) – The estimated rating $r_{ui}$\n",
    "- details (dict) – Stores additional details about the prediction that might be useful for later analysis.\n",
    "\n",
    "**=>** if you have a namedTuple `prediction`: `prediction.uid` will contain the user id. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_bl_rating_pred(user_item):\n",
    "    user = user_item[\"userId\"]\n",
    "    item = user_item[\"movieId\"]\n",
    "    \n",
    "    prediction = BaselineModel.predict(user,item)\n",
    "    \n",
    "    return prediction.est\n",
    "\n",
    "test_ratings[\"opt_bl_prediction\"] = test_ratings[[\"userId\",\"movieId\"]].apply(opt_bl_rating_pred,axis=1) \n",
    "\n",
    "test_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your turn !\n",
    "\n",
    "### __A QUICK NOTE:__ The baseline algorithm is really strong on this dataset, don't worry if the following models performance is worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Algorithm\n",
    "\n",
    "First, let's try the famous SVD algorithm, as popularized by Simon Funk during the Netflix Prize. \n",
    "\n",
    "> Matrix factorization models map both users and items ratings. to a joint latent factor space of dimensionality f, such that user-item interactions are modeled as inner products in that space.\n",
    "\n",
    "A prediction is made in the following way:\n",
    "\n",
    "## $$\\hat{r}_{ui} = \\mu + b_u + b_i + q_i^Tp_u$$\n",
    "\n",
    " When baselines are not used, this is equivalent to Probabilistic Matrix Factorization. If user u is unknown, then the bias bu and the factors pu are assumed to be zero. The same applies for item i with bi and qi.\n",
    "\n",
    "[This is the seminal paper associated to this model](https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO : Try and use the `SVD` surprise model ([related documentation page](https://surprise.readthedocs.io/en/stable/matrix_factorization.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Use the surprise SVD Implementation to predict missing ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_rating_pred(user_item):\n",
    "    user = user_item[\"userId\"]\n",
    "    item = user_item[\"movieId\"]\n",
    "    \n",
    "    prediction = ## To complete\n",
    "    \n",
    "    return ## To complete\n",
    "\n",
    "test_ratings[\"svd_prediction\"] = test_ratings[[\"userId\",\"movieId\"]].apply(svd_rating_pred,axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's compare every algorithm: the following cell should run and give you the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"mae\",\"mse\",\"rmse\"]\n",
    "results = pd.DataFrame()\n",
    "\n",
    "results[\"metrics\"] = metrics\n",
    "results[\"mean_prediction\"] = all_metrics(test_ratings[\"mean_prediction\"],test_ratings[\"rating\"])\n",
    "results[\"muser_prediction\"] = all_metrics(test_ratings[\"muser_prediction\"],test_ratings[\"rating\"])\n",
    "results[\"mitem_prediction\"] = all_metrics(test_ratings[\"mitem_prediction\"],test_ratings[\"rating\"])\n",
    "results[\"opt_bl_prediction\"] = all_metrics(test_ratings[\"opt_bl_prediction\"],test_ratings[\"rating\"])\n",
    "results[\"svd_prediction\"] = all_metrics(test_ratings[\"svd_prediction\"],test_ratings[\"rating\"])\n",
    "\n",
    "results = results.set_index(\"metrics\")\n",
    "\n",
    "print(results)\n",
    "print(\"\")\n",
    "print('---Best Models / Metrics: ---')\n",
    "results.idxmin(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing learnt embeddings\n",
    "\n",
    "> Factorizing the user-movie matrix allows us to discover the most descriptive dimensions for predicting movie preferences. We can identify the first few most important dimensions from a matrix decomposition and explore the movies’ location in this new space.\n",
    "\n",
    "To do so, we propose to use the [Tensorflow projector](https://projector.tensorflow.org/), an accessible easy to use tool online. The following functions/cells walks you through saving both the learnt movie embeddings and labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The saving function:\n",
    "\n",
    "This function saves embeddings (a numpy array) and associated labels into tsv files which can be used by the [Tensorflow projector](https://projector.tensorflow.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_embeddings(embs,dict_label,path=\"saved_word_vectors\"):\n",
    "    \"\"\"\n",
    "    embs is Numpy.array(N,size)\n",
    "    dict_label is {str(word)->int(idx)} or {int(idx)->str(word)}\n",
    "    \"\"\"\n",
    "    def int_first(k,v):\n",
    "        if type(k) == int:\n",
    "            return (k,v)\n",
    "        else:\n",
    "            return (v,k)\n",
    "\n",
    "    np.savetxt(f\"{path}_vectors.tsv\", embs, delimiter=\"\\t\")\n",
    "\n",
    "    #labels \n",
    "    if dict_label:\n",
    "        sorted_labs = np.array([lab for idx,lab in sorted([int_first(k,v) for k,v in dict_label.items()])])\n",
    "        print(sorted_labs)\n",
    "        with open(f\"{path}_metadata.tsv\",\"w\") as metadata_file:\n",
    "            for x in sorted_labs: #hack for space\n",
    "                if len(x.strip()) == 0:\n",
    "                    x = f\"space-{len(x)}\"\n",
    "                    \n",
    "                metadata_file.write(f\"{x}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Loading the movie titles.\n",
    "\n",
    "We want to label our movie with their titles (indeed, the movieId is not really informative about content)\n",
    "=> We just read the .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titleCSV = pd.read_csv(\"dataset/movies.csv\")\n",
    "titleCSV.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) We extract a mapping id => title  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2title = titleCSV[[\"movieId\",\"title\"]].set_index(\"movieId\").to_dict()[\"title\"]\n",
    "list(id2title.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) We map the inner surprise id's to the raw id's to the movie title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = data.build_full_trainset()\n",
    "index2movie = {x:id2title[full_data.to_raw_iid(x)] for x in full_data.all_items()}\n",
    "SVDmodel.qi # Holds product vectors\n",
    "SVDmodel.pu # Holds user vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Finally, we save everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings(SVDmodel.qi,index2movie,path=\"svd_items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## improving your results with GridSearch :\n",
    "\n",
    "Every machine learning model is sensible to hyperparameters: Hopefully, surprise provides a way to do parameter search [(related docs)](https://surprise.readthedocs.io/en/stable/model_selection.html#parameter-search). Here's how it work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {'n_epochs': list(range(2,5)),} #you can add parameters\n",
    "\n",
    "gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3) # you choose the algorithm and the measures\n",
    " \n",
    "gs.fit(data) # You look at best combinations (automatically deals with train/validation splits)\n",
    "\n",
    "\n",
    "# best RMSE score\n",
    "print(gs.best_score['rmse'])\n",
    "\n",
    "# combination of parameters that gave the best validation RMSE score\n",
    "print(gs.best_params['rmse'])\n",
    "\n",
    "# We can now use the algorithm that yields the best rmse:\n",
    "algo = gs.best_estimator['rmse']\n",
    "algo.fit(data.build_full_trainset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Finally, try improving your score by testing out all of surprise models and possibilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### => Use, for example, the KNNs algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.prediction_algorithms.knns import KNNBasic, KNNWithMeans\n",
    "\n",
    "KNNmodel = KNNBasic()\n",
    "KNNmodel.fit(data.build_full_trainset())\n",
    "\n",
    "def knn_rating_pred(user_item):\n",
    "    user = user_item[\"userId\"]\n",
    "    item = user_item[\"movieId\"]\n",
    "    \n",
    "    prediction = ## To complete\n",
    "    \n",
    "    return ## To complete\n",
    "\n",
    "test_ratings[\"knn_prediction\"] = test_ratings[[\"userId\",\"movieId\"]].apply(knn_rating_pred,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"mae\",\"mse\",\"rmse\"]\n",
    "results = pd.DataFrame()\n",
    "\n",
    "results[\"metrics\"] = metrics\n",
    "results[\"mean_prediction\"] = all_metrics(test_ratings[\"mean_prediction\"],test_ratings[\"rating\"])\n",
    "results[\"muser_prediction\"] = all_metrics(test_ratings[\"muser_prediction\"],test_ratings[\"rating\"])\n",
    "results[\"mitem_prediction\"] = all_metrics(test_ratings[\"mitem_prediction\"],test_ratings[\"rating\"])\n",
    "results[\"opt_bl_prediction\"] = all_metrics(test_ratings[\"opt_bl_prediction\"],test_ratings[\"rating\"])\n",
    "results[\"svd_prediction\"] = all_metrics(test_ratings[\"svd_prediction\"],test_ratings[\"rating\"])\n",
    "results[\"knn_prediction\"] = all_metrics(test_ratings[\"knn_prediction\"],test_ratings[\"rating\"])\n",
    "\n",
    "results = results.set_index(\"metrics\")\n",
    "\n",
    "print(results)\n",
    "print(\"\")\n",
    "print('---Best Models / Metrics: ---')\n",
    "results.idxmin(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still some time left: [have you tried them all ?](https://surprise.readthedocs.io/en/stable/prediction_algorithms_package.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
