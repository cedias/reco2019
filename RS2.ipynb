{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders 2 -- (45min) \n",
    "\n",
    "## Goals of this practical:\n",
    "\n",
    "1. Understand/Implement two common ranking metrics (~15min)\n",
    "2. See how SVD model perform w/ respect to those metrics (~15min)\n",
    "3. Understand the difference between implicit/explicit CF (~5min)\n",
    "3. Know and implement a really simple ranking baseline (~5min)\n",
    "4. Use a library to build an implicit recommender sysem (~5min)\n",
    "\n",
    "\n",
    "\n",
    "## Data used : [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "In this practical we use a small dataset of user ratings on movies. Specifically, we treat the dataset as list of $(user,item,rating)$ triplets.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Uncomment this to install required packages if needed (and restart kernel !)\n",
    "# ! pip install --upgrade pandas\n",
    "# ! pip install --upgrade seaborn\n",
    "# ! pip install --upgrade scikit-surprise\n",
    "# ! pip install --upgrade numpy\n",
    "# ! pip install --upgrade lightfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # For array things\n",
    "import pandas as pd # For handling data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data  & creating train/test (same as before)\n",
    "\n",
    "=> We still consider 20% as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv(\"dataset/ratings.csv\")\n",
    "ratings.head(5)\n",
    "\n",
    "train_indexes,test_indexes = [],[]\n",
    "\n",
    "for index in range(len(ratings)):\n",
    "    if index%5 == 0:\n",
    "        test_indexes.append(index)\n",
    "    else:\n",
    "        train_indexes.append(index)\n",
    "\n",
    "train_ratings = ratings.iloc[train_indexes].copy()\n",
    "test_ratings = ratings.iloc[test_indexes].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Collaborative Filtering and Ranking Metrics\n",
    "\n",
    "Most of the time recommender systems present $k$ items to the users. Therefore, recommenders are often evaluated by how many relevant items are in their $k$ set (using ranking metrics).\n",
    "\n",
    "Different ranking methods exists such as:\n",
    "- [Mean Reciprocal Rank](http://en.wikipedia.org/wiki/Mean_reciprocal_rank) \n",
    "- [normalized Discounted Cumulative Gain](https://en.wikipedia.org/wiki/Discounted_cumulative_gain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Reciprocal Rank :\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Mean_reciprocal_rank):\n",
    "\n",
    "> The mean reciprocal rank is a statistic measure for evaluating any process that produces a list of possible responses to a sample of queries, ordered by probability of correctness. The reciprocal rank of a query response is the multiplicative inverse of the rank of the first correct answer: 1 for first place, $\\frac{1}{2}$ for second place, $\\frac{1}{3}$ for third place and so on. The mean reciprocal rank is the average of the reciprocal ranks of results for a sample of queries Q\n",
    "\n",
    "$$ MRR = \\frac{1}{|Q|}\\sum^{|Q|}_{i=1}\\frac{1}{\\text{rank}_i} $$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This metric can be used to assess the mean rank of relevant suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement MRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_list = [[0,0,1],[0,1,0],[1,0,0],[0,0,0]]\n",
    "\n",
    "def rr(list_items):\n",
    "    relevant_indexes = np.asarray(list_items).nonzero()[0]\n",
    "    \n",
    "    if len(relevant_indexes) > 0:\n",
    "        \n",
    "        #NOTE:\n",
    "        # relevant_indexes[0] <= Contains the index of the 1st relevant item ([0,0,1] => 2)\n",
    "        \n",
    "        return # To Complete\n",
    "    else:\n",
    "        return # To Complete\n",
    "\n",
    "def mrr(list_list_items):\n",
    "    return # To Complete\n",
    "\n",
    "mrr(test_list) #0.4583333333333333"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (normalized) Discounted Cumulative Gain:\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "> Discounted cumulative gain (DCG) is a measure of ranking quality. In information retrieval, it is often used to measure effectiveness of web search engine algorithms or related applications. Using a graded relevance scale of documents in a search-engine result set, DCG measures the usefulness, or gain, of a document based on its position in the result list. The gain is accumulated from the top of the result list to the bottom, with the gain of each result discounted at lower ranks.\n",
    "\n",
    "$$DCG_p = \\sum^p_{i=1}\\frac{rel_i}{\\log_2{(i+1)}} = rel_1 + \\sum^p_{i=2}\\frac{rel_i}{\\log_2{(i+1)}}$$\n",
    "\n",
    ">Two assumptions are made in using DCG and its related measures.\n",
    "- Highly relevant documents are more useful when appearing earlier in a search engine result list (have higher ranks)\n",
    "- Highly relevant documents are more useful than marginally relevant documents, which are in turn more useful than non-relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Implement DCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The dcg@k is the sum of the relevance, penalized gradually\n",
    "def dcg_at_k(r, k):\n",
    "    \"\"\"Score is discounted cumulative gain (dcg)\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "        \n",
    "    \"\"\"\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return # To Complete\n",
    "        \n",
    "    return 0.\n",
    "\n",
    "# test values\n",
    "# r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "# dcg_at_k(r, 1) => 3.0\n",
    "# dcg_at_k(r, 2) => 4.2618595071429155\n",
    "r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "print(dcg_at_k(r, 1))\n",
    "print(dcg_at_k(r, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Result lists vary in length. Comparing performance from one query to the next cannot be consistently achieved using DCG alone, so the cumulative gain at each position for a chosen value of p should be normalized. The normalized discounted cumulative gain, or nDCG, is computed as: \n",
    "\n",
    "$$ nDCG_p = \\frac{DCG_p}{IDCG_p} $$\n",
    "\n",
    "> Where IDCG_p is the maximum possible DCG through position p, also called Ideal DCG (IDCG). It is obtained by sorting all relevant documents in the corpus by their relative relevance and computing the DCG:\n",
    "\n",
    "$$ IDCG_p = max(DCG_p) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement NDCG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And it's normalized version\n",
    "def ndcg_at_k(r, k):\n",
    "    \"\"\"\n",
    "        r: Relevance scores (list or numpy) in rank order\n",
    "            (first element is the first item)\n",
    "        k: Number of results to consider\n",
    "    \"\"\"\n",
    "    dcg_max =  # To Complete\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    return # To Complete\n",
    "\n",
    "# test values\n",
    "# r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]\n",
    "# ndcg_at_k(r, 1) => 1.0\n",
    "# ndcg_at_k(r, 4) => 0.794285\n",
    "    \n",
    "r = [3, 2, 3, 0, 0, 1, 2, 2, 3, 0]    \n",
    "ndcg_at_k(r, 4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now, let's see how SVD performs on these metrics:\n",
    "\n",
    "Here, we're not in the same evaluation framework as in the last session. The goal for our recommender is to present $k$ items to the users instead of simply scoring $(user,item)$ pairs. We can still use previous models if we consider the predicted rating as a way to rank item to present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  To make a recommender we also need two `{userId: [movieId, movieId, ...] }` dictionnaries:\n",
    "\n",
    "- `already_seen`: Items that were already seen by users. This is for training and not recommending them again\n",
    "- `ground_truth`: Items that will be seen and liked (rating >= 5) by users. This is our ground truth to evaluate our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_seen = (\n",
    "    train_ratings \n",
    "    .groupby(\"userId\")[\"movieId\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    "    )\n",
    "\n",
    "ground_truth = (\n",
    "    test_ratings[test_ratings.rating >= 5] \n",
    "    .groupby(\"userId\")[\"movieId\"]\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We also need the set of all items that can be recommended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_items = set(train_ratings[\"movieId\"].unique())\n",
    "print(\"The recommender system will have to pick a few items from\",len(existing_items),\"possible items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Ok, now, let's make a quick surprise SVD recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "data = Dataset.load_from_df(train_ratings[['userId', 'movieId', 'rating']], Reader(rating_scale=(1, 5)))\n",
    "model = # To Complete\n",
    "\n",
    "# fit the model ?\n",
    "# To Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_rating_pred(user_item):\n",
    "    user = user_item[\"userId\"]\n",
    "    item = user_item[\"movieId\"]\n",
    "    \n",
    "    prediction = model.predict(user,item)\n",
    "    \n",
    "    return prediction.est\n",
    "\n",
    "test_ratings[\"svd_prediction\"] = test_ratings[[\"userId\",\"movieId\"]].apply(svd_rating_pred,axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse = ((test_ratings[\"rating\"] - test_ratings[\"svd_prediction\"])**2).mean()\n",
    "mae = ((test_ratings[\"rating\"] - test_ratings[\"svd_prediction\"]).abs()).mean()\n",
    "\n",
    "print(f\"MSE: {mse} -- MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_rating_pred(model,user,item):\n",
    "    prediction = # To Complete\n",
    "    return # To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's create the relevance list for our MRR function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_rel = []\n",
    "    \n",
    "\n",
    "for user,will_see in ground_truth.items():\n",
    "    rel_list = []\n",
    "    will_see = set(will_see)\n",
    "    has_seen = set(already_seen[user])\n",
    "    can_see = [(mid,model_rating_pred(model,user,mid)) for mid in existing_items - has_seen]\n",
    "    \n",
    "    \n",
    "    for movie,score in reversed(sorted(can_see,key=lambda x:x[1])):\n",
    "        if movie in will_see:\n",
    "            rel_list.append(1)\n",
    "            break\n",
    "        else:\n",
    "            rel_list.append(0)        \n",
    "    rel_list[-1] = 1 # when no relevant item exist\n",
    "    list_of_rel.append(rel_list)\n",
    "    \n",
    "\n",
    "svd_mrr = mrr(list_of_rel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"On average, the {int(round(1/svd_mrr,0))}th proposed item is relevant (on {len(existing_items)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our result is the following :\n",
    "> 'On average, the 9th proposed item is relevant (on 8970)'\n",
    "\n",
    "So, on average, if you show around 10 items to someone, there will be at least one which will be relevant: not bad for a guess on 8970 !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need something better to compare though:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A good enough implicit baseline: popular items\n",
    "What if you only show popular items to people ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) let's find the popular items:\n",
    "\n",
    "- By popular we mean the \"most\" rated movies (the one with the more rating)\n",
    "\n",
    "should be something along this: \n",
    "[318,\n",
    " 356,\n",
    " 296,\n",
    " 2571,\n",
    " 593,\n",
    " 110,\n",
    " 480,\n",
    " 260,\n",
    " 589,\n",
    " 527,\n",
    " 780,\n",
    " ...\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_counts = # To Complete\n",
    "popular_item_list = # To Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(popular_item_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_items = set(popular_item_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Building the popular recommendation relevance list per user.\n",
    "\n",
    "To use our MRR metric function, we need to build a list of lists containing 0's or 1's. There is one list per user.\n",
    "0 means not relevant, 1 means relevant. We need to score each item until the first relevant one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_rel = []\n",
    "\n",
    "for user,will_see in ground_truth.items():\n",
    "    rel_list = []\n",
    "    will_see = set(will_see)\n",
    "    has_seen = set(already_seen[user])\n",
    "    \n",
    "    for movie in popular_item_list:\n",
    "        if movie in has_seen:         # User has already seen movie -> Can filter prediction\n",
    "            continue\n",
    "        elif movie in will_see:       # User will see, spot on suggestion !         \n",
    "            rel_list.append # To Complete\n",
    "            break\n",
    "        else:                         # No clue.\n",
    "            rel_list.append # To Complete\n",
    "            \n",
    "    if rel_list[-1] == 1:             # when no relevant item exist, no need to take it into account.\n",
    "        list_of_rel.append(rel_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_mrr = mrr(list_of_rel)\n",
    "f\"On average, the {int(round(1/pop_mrr,0))}th proposed item is relevant (on {len(existing_items)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our result is the following :\n",
    "> 'On average, the 4th proposed item is relevant (on 8970)'\n",
    "\n",
    "Ok, the popular baseline is WAY better than our SVD model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Collaborative Filtering \n",
    "\n",
    "Implicit collaborative filtering, unlike explicit CF, is the task of learning from interactions (not ratings). The models are quite close but mainly differ by the optimized loss.\n",
    "\n",
    "\n",
    "\n",
    "Here, we propose to use the [lightFM](https://github.com/lyst/lightfm) model to do implicit collaborative filtering.\n",
    "\n",
    "\n",
    "> LightFM is a Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback, including efficient implementation of BPR and WARP ranking losses. It's easy to use, fast (via multithreaded model estimation), and produces high quality results.\n",
    "\n",
    "LightFM is a simple matrix factorization algorithm where a rating is predicted in the following way:\n",
    "\n",
    "$$\\hat{r_{ui}} = f(q_u · p_i + b_u + b_i)$$\n",
    "\n",
    "Where $q_u$,$p_i$ and $b_u$, $b_i$ are respectively user and item latent profile and features\n",
    "\n",
    "### What are interactions :\n",
    "\n",
    "Interactions can be any type (clicks, pause, gaze, comment, review) it's a lot more versatile than ratings. Here we'll consider two setups:\n",
    "\n",
    "- Interactions are ratings >= 5\n",
    "- Interactions are every ratings \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) Let's create the interaction train/test dataset within the framework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Relevant Documentation](http://lyst.github.io/lightfm/docs/lightfm.data.html)\n",
    "\n",
    "To create a dataset: \n",
    "- (a) Create an instance of the Dataset class.\n",
    "- (b) Call fit (or fit_partial), supplying user/item ids and feature names that you want to use in your model. This will create internal mappings that translate the ids and feature names to internal indices used by the LightFM model.\n",
    "- (c) Call build_interactions with an iterable of (user id, item id) or (user id, item id, weight) to build an interactions and weights matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm.data import Dataset\n",
    "\n",
    "# (a) Create a dataset\n",
    "dataset = Dataset()\n",
    "\n",
    "\n",
    "# (b) Create an internal mapping for users and items (We need to consider train + test)\n",
    "dataset.fit((x for x in ratings[\"userId\"]),\n",
    "            (x for x in ratings['movieId']))\n",
    "\n",
    "# (c) Create the interaction matrices\n",
    "(train_interactions, weights) = dataset.build_interactions(\n",
    "    ((x.userId, x.movieId) for x in train_ratings.itertuples() if x.rating >= 5) # We only consider 5's as interactions\n",
    ") \n",
    "(test_interactions, weights) = dataset.build_interactions(\n",
    "    ((x.userId, x.movieId) for x in test_ratings.itertuples() if x.rating >= 5)  # We only consider 5's as interactions\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Create and train the lightFM model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model learns embeddings (latent representations in a high-dimensional space) for users and items in a way that encodes user preferences over items. When multiplied together, these representations produce scores for every item for a given user; items scored highly are more likely to be interesting to the user.\n",
    "\n",
    "Four loss functions are available:\n",
    "\n",
    "- logistic: useful when both positive (1) and negative (-1) interactions are present.\n",
    "- BPR: Bayesian Personalised Ranking [1] pairwise loss. Maximises the prediction difference between a positive example and a randomly chosen negative example. Useful when only positive interactions are present and optimising ROC AUC is desired.\n",
    "- WARP: Weighted Approximate-Rank Pairwise [2] loss. Maximises the rank of positive examples by repeatedly sampling negative examples until rank violating one is found. Useful when only positive interactions are present and optimising the top of the recommendation list (precision@k) is desired.\n",
    "- k-OS WARP: k-th order statistic loss [3]. A modification of WARP that uses the k-th positive example for any given user as a basis for pairwise updates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm import LightFM\n",
    "\n",
    "model = LightFM(loss='bpr',random_state=50000)\n",
    "model.fit(train_interactions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Evaluation of learnt model with lightfm\n",
    "\n",
    "[Relevant documentation](http://lyst.github.io/lightfm/docs/lightfm.evaluation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightfm.evaluation import reciprocal_rank\n",
    "bpr_mrr = reciprocal_rank(model, test_interactions, train_interactions).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"On average, the {int(round(1/bpr_mrr,0))}th proposed item is relevant (on {len(existing_items)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we get\n",
    "> 'On average, the 6th proposed item is relevant (on 8970)'\n",
    "\n",
    "It's much better than the SVD, but still worse than the popular baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Let's do the same, but now we consider EVERY rating as one interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the interaction matrix\n",
    "(train_interactions_all, weights) = # To Complete\n",
    "(test_interactions_all, weights) = # To Complete\n",
    "\n",
    "from lightfm import LightFM\n",
    "\n",
    "\n",
    "model_bpr_all = LightFM(loss='bpr',random_state=50000)\n",
    "model_bpr_all.fit(train_interactions_all)\n",
    "\n",
    "bpr_mrr_all = reciprocal_rank(model_bpr_all, test_interactions_all, train_interactions_all).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"On average, the {int(round(1/bpr_mrr_all,0))}th proposed item is relevant (on {len(existing_items)})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got\n",
    "> 'On average, the 3th proposed item is relevant (on 8970)'\n",
    "\n",
    "Much better !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some time left ? Try and experiment with every lightfm losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
