{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommenders 4 : Pytorch and Recommenders (~1h)\n",
    "\n",
    "In this practical session, we dive a little more into [pytorch](https://pytorch.org/docs/stable/index.html) and propose to re-implement two classical matrix-factorization models with this neural network toolkit.\n",
    "\n",
    "## Objectives:\n",
    "\n",
    "- (a) See a bit of simple pytorch (~5min)\n",
    "- (b) Discover the \"autograd\" part of pytorch to build a simple baseline (~20min)\n",
    "- (c) Discover the \"nn\" part of pytorch to build a simple matrix factorization algorithm (~20min)\n",
    "- (d) Learn to use a high level framework for pytorch (kind of \"KERAS\" like) to build more complicated algorithms (~15min)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch torchvision pytorch-lightning --upgrade\n",
    "#! pip install matplotlib --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a) WHAT IS PYTORCH?\n",
    "\n",
    "It’s a Python-based scientific computing package targeted at two sets of audiences:\n",
    "\n",
    "- A replacement for NumPy to use the power of GPUs\n",
    "- a deep learning research platform that provides maximum flexibility and speed\n",
    "\n",
    "### Tensors : the main unit\n",
    "\n",
    "Tensors are similar to NumPy’s ndarrays, with the addition being that Tensors can also be used on a GPU to accelerate computing.\n",
    "\n",
    "\n",
    "## Some useful functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### initialize an empty 4x2 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_empty = torch.empty(4, 2)\n",
    "print(x_empty)  #Tensor is not initialized => contains gibberish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a 3x2 tensor filled with zeros of type long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = torch.zeros(3, 2, dtype=torch.long)\n",
    "print(x0) #Tensor has only zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a tensor of size 2 with (0 => 5.5) and (1 => 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor([5.5, 3])\n",
    "print(x_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.tensor(np.array([5.5, 3])) #also works with numpy arrays\n",
    "print(x_data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create random 5x3 and 3x5 tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(5,3)\n",
    "y = torch.rand(3,5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing works just like numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[:,1] #The 2nd column (indexing starts at 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[[1,3],:] # the 2nd and 4th row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = torch.tensor([1])\n",
    "print(scalar.item()) # Gets the value when tensor is a scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### know the size of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.size() ## equivalent to x.shape in numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### simple addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x+1        # same as x.add(1)\n",
    "x.add_(1)  # inplace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mm(x,y) # same as x @ y or np.dot(x.numpy(),y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to understand:\n",
    "\n",
    "Pytorch can be a drop-in replacement for numpy. It behaves mostly the same and the API is close.\n",
    "\n",
    "\n",
    "### There are many more creation/operation ops:\n",
    "\n",
    "=> You can have a look at the [torch.Tensor documentation](https://pytorch.org/docs/stable/tensors.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's interesting beyond the \"numpy replacement\": autodiff !\n",
    "\n",
    "Pytorch has Automatic differentiation: You only have to compute a loss function to obtain gradients automatically. How it works is detailed [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html#pytorch-tensors-and-autograd)\n",
    "\n",
    "### Let's do 1d-linear regression with the vanilla autodiff !\n",
    "\n",
    "#### (First) we need fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_x = np.arange(0,100,1)\n",
    "fake_y = np.arange(0,10,0.1) + np.random.rand(100)\n",
    "plt.plot(fake_x,fake_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Here, we won't split the data in train/val/test, this is just an example\n",
    "\n",
    "So, the linear model we want to learn is the following:\n",
    "$$f(x) =  wx+b $$\n",
    "The parameters to optimize are w and b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create to tensor variables (which are our parameters)\n",
    "w = torch.tensor([1.],requires_grad=True) # We need to set requires_grad to True so the gradient can flow.\n",
    "b = torch.tensor([0.5],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the f function:\n",
    "def f(x):\n",
    "    return (w*x)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define an error function (here the MAE)\n",
    "def error(pred,real):\n",
    "    return (pred-real).abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, we cycle through the data, optimizing the parameters with respect to the gradient of the error:\n",
    "plt.plot(fake_y)\n",
    "\n",
    "for epoch in range(4): # We cycle 4 times\n",
    "    mean_loss = 0\n",
    "    \n",
    "    for x,y in zip(fake_x,fake_y): \n",
    "        \n",
    "        pred = f(x) #predict\n",
    "        loss = error(pred,y) #compute loss\n",
    "        loss.backward() # This does backpropagation and sets .grad attribute.\n",
    "\n",
    "        # Update parameters via SGD:\n",
    "        with torch.no_grad(): # This deactivated gradient calculations\n",
    "            \n",
    "            mean_loss += loss.item() # get the raw value of a (1,) tensor\n",
    "            w -= 0.0001 * w.grad # This wouldn't be possible w/ gradient (-= is an inplace operation)\n",
    "            b -= 0.0001 * b.grad\n",
    "            w.grad.zero_()\n",
    "            b.grad.zero_()\n",
    "            \n",
    "    # Plot the resulting line        \n",
    "    predictions = [f(x) for x in range(100)]\n",
    "    plt.plot(predictions,label=f\"epoch {epoch}\")\n",
    "\n",
    "    print('----')\n",
    "    print(\"loss:\", mean_loss/len(fake_y))\n",
    "    print('w:',w.item())\n",
    "    print('b:',b.item())\n",
    "    \n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Full pytorch tutorial: \n",
    "\n",
    "A tutorial can be found [here](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) do not hesitate to take a couple of minutes to skim read it. Plenty of [ressources](https://pytorch.org/resources) are available online. Also, you can have a look at the [extensive pytorch documentation](https://pytorch.org/docs/stable/index.html). \n",
    "\n",
    "Here, as we are defining neural networks, we mainly use the `torch.nn` module which contains most classical deep learning building blocks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data used : [smallest movie-lens dataset](https://grouplens.org/datasets/movielens/)\n",
    "\n",
    "=> Just like the previous sessions\n",
    "\n",
    "\n",
    "# 1)  Load & Prepare Data\n",
    "\n",
    "To be able to embed the data easily, we need to remap  the user/items between [0->N_User] and [0->N_Items]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "## Load\n",
    "ratings = pd.read_csv(\"dataset/ratings.csv\")\n",
    "ratings.head(5)\n",
    "\n",
    "## Prepare Data\n",
    "user_map = {user:num for num,user in enumerate(ratings[\"userId\"].unique())}\n",
    "item_map = {item:num for num,item in enumerate(ratings[\"movieId\"].unique())}\n",
    "\n",
    "## Number of users & items\n",
    "num_users = len(user_map)\n",
    "num_items = len(item_map)\n",
    "\n",
    "ratings[\"userId\"] = ratings[\"userId\"].map(user_map)\n",
    "ratings[\"movieId\"] = ratings[\"movieId\"].map(item_map)\n",
    "\n",
    "ratings.head(5)\n",
    "\n",
    "# Creating Test/Train as before\n",
    "\n",
    "train_indexes,val_indexes,test_indexes = [],[],[]\n",
    "\n",
    "for index in range(len(ratings)):\n",
    "    if index%5 == 0:\n",
    "        test_indexes.append(index)\n",
    "    else:\n",
    "        train_indexes.append(index)\n",
    "\n",
    "        \n",
    "shuffle(train_indexes)\n",
    "num_val = int(len(train_indexes)/100*20)\n",
    "val_indexes = train_indexes[:num_val]\n",
    "train_indexes = train_indexes[num_val:]\n",
    "\n",
    "train_ratings = ratings.iloc[train_indexes].copy()\n",
    "val_ratings = ratings.iloc[val_indexes].copy()\n",
    "test_ratings = ratings.iloc[test_indexes].copy()\n",
    "\n",
    "\n",
    "print(f\" #train:{len(train_ratings)}, #val:{len(val_ratings)} ,#test:{len(test_ratings)}\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Reproduce the baseline model with pytorch's vanilla autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal now is to reproduce the following baseline model from surprise\n",
    "\n",
    "## $$\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) : First, let's define the parameters\n",
    "\n",
    "You have many parameters, they are all 1-dimensional:\n",
    "- **mu:** the global mean (1,)\n",
    "- **bu:** the user means (n_users,)\n",
    "- **bi:** the item means (n_items,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = #To Complete\n",
    "bu = [ #To Complete for _ in range(num_users)]\n",
    "bi = [ #To Complete for _ in range(num_items)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Then, we define two functions: \n",
    "\n",
    "- `predict(u,i)` : Will return the prediction given the (user,item) pair\n",
    "- `error(pred,real)` : Will return the MSE error of prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (TODO) Predict Function\n",
    "This function should implement this: $\\hat{r}_{ui} = b_{ui} = \\mu + b_u + b_i$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(u,i):\n",
    "    \n",
    "    if u < num_users: # if user exist:\n",
    "        user_mean = #To Complete\n",
    "    else:\n",
    "        user_mean = #To Complete\n",
    "        \n",
    "    if i < num_items: # if item exist:\n",
    "        item_mean = #To Complete\n",
    "    else:\n",
    "        item_mean = #To Complete\n",
    "        \n",
    "    return #To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) error function\n",
    "We want to use the MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(pred,real):\n",
    "    return #To Complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The evaluation loop, without any optimization for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_e = 0\n",
    "for index, uid, mid, r, ts in train_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    train_e += error(result,r).item()\n",
    "    \n",
    "val_e = 0\n",
    "for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    val_e += error(result,r).item()\n",
    "\n",
    "test_e = 0\n",
    "for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "    result = predict(uid,mid)\n",
    "    test_e += error(result,r).item()\n",
    "\n",
    "print(\"final train error : \", train_e/len(train_ratings))\n",
    "print(\"final val error : \", val_e/len(val_ratings))\n",
    "print(\"final test error : \", test_e/len(test_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's optimize the parameters (with SGD)  by slightly modifying the previous loop\n",
    "\n",
    "### (TODO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.01\n",
    "batch_size = 32\n",
    "n_epochs = 5\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    \n",
    "    train_error = 0\n",
    "    \n",
    "    for num,(index, uid, mid, r, ts) in enumerate(train_ratings.sample(frac=1).itertuples()):\n",
    "        result = #To Complete\n",
    "        ex_error = #To Complete\n",
    "        train_error += #To Complete\n",
    "\n",
    "        if num % batch_size == 0:\n",
    "            ex_error.backward()\n",
    "\n",
    "\n",
    "            with torch.no_grad():\n",
    "                mu -= #To Complete\n",
    "                bu[uid] -= #To Complete\n",
    "                bi[mid] -= #To Complete\n",
    "\n",
    "                # Manually zero the gradients after updating weights\n",
    "                mu.grad.zero_()\n",
    "                bu[uid].grad.zero_()\n",
    "                bi[mid].grad.zero_()\n",
    "\n",
    "\n",
    "    print(f\"epoch {epoch} train error : \", train_e/len(train_ratings))\n",
    "    \n",
    "    val_e = 0\n",
    "    for index, uid, mid, r, ts in val_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        val_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} val error : \", val_e/len(val_ratings))\n",
    "\n",
    "\n",
    "    test_e = 0\n",
    "    for index, uid, mid, r, ts in test_ratings.itertuples():\n",
    "        result = predict(uid,mid)\n",
    "        test_e += error(result,r).item()\n",
    "\n",
    "    print(f\"epoch {epoch} test error : \", test_e/len(test_ratings))\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Pytorch (.nn) Modules\n",
    "\n",
    "Instead of having to define everything by hand, pytorch has several usefull abstractions:\n",
    "\n",
    "- `nn.Module()` -> To define the model and the forward computation\n",
    "- `torch.utils.data.DataLoader` -> To create the data pipeline\n",
    "\n",
    "To explore these modules, we'll do the following model:\n",
    "\n",
    "##  Classic SVD (with mean)\n",
    "\n",
    "To see how it works, we propose to implement a simple SVD:\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu) }_\\text{regularization} $$\n",
    "\n",
    "where prediction is done in the following way:\n",
    "### $$r_{ui} = \\mu + U_u.I_i $$\n",
    "\n",
    "where $\\mu$ is the global mean,  $U_u$ a user embedding and $I_i$ an item embedding\n",
    "\n",
    "### STEPS:\n",
    " To implement such model in pytorch, we need to do multiple things:\n",
    " \n",
    " - (1) model definition\n",
    " - (2) loss function\n",
    " - (3) evaluation\n",
    " - (4) training/eval loop\n",
    "\n",
    "\n",
    "#### (1) Model definition\n",
    "\n",
    "A model class typically extends `nn.Module`, the Neural network module. It is a convenient way of encapsulating parameters, with helpers for moving them to GPU, exporting, loading, etc.\n",
    "\n",
    "One should define two functions: `__init__` and `forward`.\n",
    "\n",
    "- `__init__` is used to initialize the model parameters\n",
    "- `forward` is the net transformation from input to output. In fact, when doing `moduleClass(input)` you call this method.\n",
    "\n",
    "##### (a) Initialization\n",
    "\n",
    "Our model has different weigths:\n",
    "\n",
    "- the user profiles (also called user embeddings) $U$\n",
    "- the item profiles (also called user embeddings) $I$\n",
    "- the mean bias $\\mu$\n",
    "\n",
    "\n",
    "##### (b) input to output operation\n",
    "Technically, the prediction as defined earlier can be seen as just a dot product between two embeddings $U_u$ and $I_i$ plus the mean rating:\n",
    "\n",
    "- `torch.sum(embed_u*embed_i,1) + self.mean` is equivalent to $r_{ui} = \\mu + U_u.I_i $ \n",
    "- the `.squeeze(1)` operation is a shape operation to remove the dimension 1 (indexing starts at 0) akin to reshaping the matrix from `(batch_size,1,latent_size)` to `(batch_size,latent_size)`\n",
    "- for reference, the inverse operation is `.unsqueeze()`\n",
    "- we return weights to regularize them\n",
    "\n",
    "\n",
    "### (TODO) Just to make sure you were following: complete the following `forward` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's create the datasets following  (Object w/ __getitem__(index) and __len()__, i.e lists ;)\n",
    "prep_train = [(tp.userId,tp.movieId,tp.rating) for tp in train_ratings.itertuples()]\n",
    "prep_val = [(tp.userId,tp.movieId,tp.rating) for tp in val_ratings.itertuples()]\n",
    "prep_test = [(tp.userId,tp.movieId,tp.rating) for tp in test_ratings.itertuples()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# The model define as a class, inheriting from nn.Module\n",
    "class ClassicMF(nn.Module):\n",
    "    \n",
    "    #(a) Init\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(ClassicMF, self).__init__()\n",
    "        \n",
    "        #Embedding layers\n",
    "        self.users = nn.Embedding(#To Complete, latent_size)        \n",
    "        self.items = nn.Embedding(#To Complete, latent_size)\n",
    "        #The mean bias\n",
    "        self.mean = nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "        \n",
    "        #initialize weights with very small values\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "\n",
    "    \n",
    "    # (b) How we compute the prediction (from input to output)\n",
    "    def forward(self, user, item): ## method called when doing ClassicMF(user,item)\n",
    "        \n",
    "        embed_u,embed_i = self.users(user).squeeze(1),self.items(item).squeeze(1)\n",
    "        out =  #To Complete\n",
    "\n",
    "        return out, embed_u, embed_i, self.mean  # We return prediction + weights to regularize them\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2-4) full train loop\n",
    "\n",
    "The train loop is organized around the [Dataloader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader) class which Combines a dataset and a sampler, and provides single- or multi-process iterators over the dataset.\n",
    "\n",
    "We just redefine a collate function\n",
    "\n",
    "> collate_fn (callable, optional) – merges a list of samples to form a mini-batch.\n",
    "\n",
    "\n",
    "**NOTE:** The dataset argument can be a list instead of a \"Dataset\" instance (works by duck typing)\n",
    "    \n",
    "\n",
    "##### The train loop sequence is the following:\n",
    "    \n",
    "[Dataset ==Dataloader==> Batch (not prepared) ==collate_fn==> Batch (prepared) ==Model.forward==> Prediction =loss_fn=> loss <-> truth \n",
    "\n",
    "1] PREDICT\n",
    "- (a) The dataloader samples training exemples from the dataset (which is a list)\n",
    "- (b) The collate_fn prepares the minibatch of training exemples\n",
    "- (c) The prediction is made by feeding the minibatch in the model\n",
    "- (d) The loss is computed on the prediction via a loss function\n",
    "\n",
    "2] OPTIMIZE\n",
    "- (e) Gradients are computed by automatic backard propagation\n",
    "- (f) Parameters are updated using computed gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# HyperParameters\n",
    "n_epochs = 3\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "#(b) Collate function => Creates tensor batches to feed model during training\n",
    "# It can be removed if data is already tensors (torch or numpy ;)\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items, ratings = zip(*l) \n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "\n",
    "#(d) Loss function => Combines MSE and L2\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction='sum')\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "#\n",
    "# Training script starts here\n",
    "#    \n",
    "\n",
    "\n",
    "model = ClassicMF(num_users,num_items,num_feat)\n",
    "\n",
    "\n",
    "\n",
    "# (a) dataloader will sample data from datasets using collate_fn tuple_batch\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train loop\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    ## Training loss (the one we train with)\n",
    "    \n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train() # set the model on train mode\n",
    "        model.zero_grad() # reset gradients\n",
    "        \n",
    "        #(c) predictions are made by the model\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        \n",
    "        #(d) loss computed on predictions, we added regularization\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        \n",
    "        loss.backward() #(e) backpropagating to get gradients\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step() #(f) updating parameters\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        ## Validation loss (no training)\n",
    "        for users_t,items_t,ratings_t in dataloader_val:\n",
    "\n",
    "            model.eval() # Inference mode\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[1] += mse_loss    \n",
    "\n",
    "        ## Test loss (no training)\n",
    "\n",
    "        for users_t,items_t,ratings_t in dataloader_test:\n",
    "            model.eval()\n",
    "            pred,*params = model(users_t,items_t)\n",
    "            _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "\n",
    "            mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Your turn) Koren 2009 model:\n",
    "\n",
    "Here, this model simply adds a bias for each user and for each item\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "### TODO:\n",
    "\n",
    "- (a) complete the model initialization\n",
    "- (b) complete the forward method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KorenMF(nn.Module):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(KorenMF, self).__init__()\n",
    "        \n",
    "        self.users = #To Complete\n",
    "        self.items = #To Complete\n",
    "        self.umean = #To Complete\n",
    "        self.imean = #To Complete\n",
    "        self.gmean =  #To Complete\n",
    "\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "        nn.init.normal_(self.umean.weight,2,1)\n",
    "        nn.init.normal_(self.imean.weight,2,1)\n",
    "        \n",
    "        \n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        out = #To Complete\n",
    "\n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (TODO) Here, train loop stays the same, you only have to change the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "n_epochs = 10\n",
    "batch_size = 16\n",
    "num_feat = 25\n",
    "lr = 0.01\n",
    "reg = 0.001\n",
    "\n",
    "\n",
    "\n",
    "def tuple_batch(l):\n",
    "    '''\n",
    "    input l: list of (user,item,review, rating tuples)\n",
    "    output: formatted batches (in torch tensors)\n",
    "\n",
    "    takes n-tuples and create batch\n",
    "    text -> seq word #id\n",
    "    '''\n",
    "    users, items,ratings = zip(*l)\n",
    "    users_t = torch.LongTensor(users)\n",
    "    items_t = torch.LongTensor(items)\n",
    "    ratings_t = torch.FloatTensor(ratings)\n",
    "    \n",
    "    return users_t,items_t,ratings_t\n",
    "\n",
    "\n",
    "def loss_func(pred,ratings_t,reg,*params):\n",
    "    '''\n",
    "    mse loss combined with l2 regularization.\n",
    "    params assumed 2-dimension\n",
    "    '''\n",
    "    mse = F.mse_loss(pred,ratings_t,reduction=\"sum\")\n",
    "    l2 = 0\n",
    "    for p in params:\n",
    "        l2 += torch.mean(p.norm(2,-1))\n",
    "        \n",
    "    return (mse/pred.size(0)) + reg*l2 , mse\n",
    "    \n",
    "\n",
    "model =  #To Complete\n",
    "\n",
    "\n",
    "dataloader_train = DataLoader(prep_train, batch_size=batch_size, shuffle=True, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_val = DataLoader(prep_val, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "dataloader_test = DataLoader(prep_test, batch_size=batch_size, shuffle=False, num_workers=0, collate_fn=tuple_batch)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "\n",
    "for e in range(n_epochs):\n",
    "    mean_loss = [0,0,0] #train/val/test\n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_train:\n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "\n",
    "        loss,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "        loss.backward()\n",
    "        \n",
    "        mean_loss[0] += mse_loss\n",
    "        optimizer.step()\n",
    "    \n",
    "    \n",
    "\n",
    "    for users_t,items_t,ratings_t in dataloader_val:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[1] += mse_loss    \n",
    "        \n",
    "    for users_t,items_t,ratings_t in dataloader_test:\n",
    "        model.eval()\n",
    "        pred,*params = model(users_t,items_t)\n",
    "        _,mse_loss = loss_func(pred,ratings_t,reg,*params)\n",
    "    \n",
    "        mean_loss[2] += mse_loss    \n",
    "\n",
    "    print(\"-\"*25)\n",
    "    print(\"epoch\",e, \"mse (train/val/test)\", round((mean_loss[0]/len(prep_train)).item(),3),\"/\",  round((mean_loss[1]/len(prep_val)).item(),3),\"/\",  round((mean_loss[2]/len(prep_test)).item(),3))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch's keras: Pytorch-Lightning\n",
    "\n",
    "Pytorch lightning is a easy to use framework for Pytorch. To start a new project you just need to define two files:\n",
    "\n",
    "- a LightningModule (which inherits `pl.LightningModule`)\n",
    "- a Trainer file. \n",
    "\n",
    "By defining those two files, you get:\n",
    "- Checkpointing\n",
    "- Debugging\n",
    "- Distributed training\n",
    "- Experiment Logging\n",
    "- Training loop\n",
    "- Validation loop\n",
    "- Testing loop\n",
    "\n",
    "## Let's try with the same but different Koren 2009 model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### $$r_{ui} = \\mu + \\mu_i + \\mu_u + U_u.I_i $$\n",
    "\n",
    "Where the goal is to minimize the following loss\n",
    "\n",
    "### $$ \\min\\limits_{U,I}\\sum\\limits_{(u,i)} \\underbrace{(r_{ui} -  (I_i^TU_u + \\mu+ \\mu_i+\\mu_u))^2}_\\text{minimization} + \\underbrace{\\lambda(||U_u||^2+||I_u||^2 + \\mu  + \\mu+ \\mu_i+\\mu_u) }_\\text{regularization} $$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Complete the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "class LightningKorenMF(pl.LightningModule):\n",
    "\n",
    "    def __init__(self,nb_users,nb_items,latent_size):\n",
    "        super(LightningKorenMF, self).__init__()\n",
    "        \n",
    "        self.reg = 0.001\n",
    "        \n",
    "        self.users = #To Complete\n",
    "        self.items = #To Complete\n",
    "        self.umean = #To Complete\n",
    "        self.imean = #To Complete\n",
    "        self.gmean =  nn.Parameter(torch.FloatTensor(1,).fill_(3))\n",
    "\n",
    "        nn.init.normal_(self.users.weight,0,0.01)\n",
    "        nn.init.normal_(self.items.weight,0,0.01)\n",
    "        nn.init.normal_(self.umean.weight,0.1,0.1)\n",
    "        nn.init.normal_(self.imean.weight,0.1,0.1)\n",
    "        \n",
    "\n",
    "    def forward(self, user,item):\n",
    "        embed_u,embed_i = self.users(user).squeeze(1) , self.items(item).squeeze(1)\n",
    "        umean, imean = self.umean(user) , self.imean(item)\n",
    "        out = #To Complete\n",
    "\n",
    "        return out , embed_u, embed_i, umean , imean , self.gmean\n",
    "\n",
    "    \n",
    "    def my_loss_func(self, pred,ratings_t,reg,*params):\n",
    "        '''\n",
    "        mse loss combined with l2 regularization.\n",
    "        params assumed 2-dimension\n",
    "        '''        \n",
    "        mse = F.mse_loss(pred,ratings_t)\n",
    "        l2 = 0\n",
    "        for p in params:\n",
    "            l2 += torch.mean(p.norm(2,-1))\n",
    "\n",
    "        return mse + reg*l2 , mse\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        # REQUIRE\n",
    "        users_t,items_t,ratings_t = batch\n",
    "        pred , *params = self.forward(users_t,items_t) \n",
    "        loss,mse = self.my_loss_func(pred,ratings_t,self.reg,*params)\n",
    "\n",
    "        return {'loss':loss,\"mse\":mse}\n",
    "    \n",
    "\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        return {\"val_mse\":self.training_step(batch,batch_nb)[\"mse\"]}\n",
    "    \n",
    "    def validation_end(self,outputs):\n",
    "        return {\"progress_bar\":{\"val_mse\":torch.tensor([output['val_mse'] for output in outputs]).mean().item()}}\n",
    "    \n",
    "    def test_step(self, batch, batch_nb):\n",
    "        return {\"test_mse\":self.training_step(batch,batch_nb)[\"mse\"]}\n",
    "    \n",
    "    def test_end(self,outputs):\n",
    "        res = {\"progress_bar\":{\"test_mse\":torch.tensor([output['test_mse'] for output in outputs]).mean().item()}}\n",
    "        print(res)\n",
    "        return res\n",
    "\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # REQUIRED\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.002)\n",
    "    \n",
    "    def tuple_batch(self,l):\n",
    "        '''\n",
    "        input l: list of (user,item,rating tuples)\n",
    "        output: formatted batches (in torch tensors)\n",
    "\n",
    "        takes n-tuples and create batch\n",
    "        text -> seq word #id\n",
    "        '''\n",
    "        users, items, ratings = zip(*l) \n",
    "        users_t = torch.LongTensor(users)\n",
    "        items_t = torch.LongTensor(items)\n",
    "        ratings_t = torch.FloatTensor(ratings)\n",
    "\n",
    "        return users_t, items_t, ratings_t\n",
    "    \n",
    "\n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        # REQUIRED\n",
    "        return DataLoader(prep_train,collate_fn=self.tuple_batch ,num_workers=0, batch_size=32)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(prep_val,collate_fn=self.tuple_batch,num_workers=0, batch_size=32)\n",
    "\n",
    "    @pl.data_loader\n",
    "    def test_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        return DataLoader(prep_test,collate_fn=self.tuple_batch,num_workers=0, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "\n",
    "model = LightningKorenMF(num_users,num_items,50)\n",
    "\n",
    "# most basic trainer, uses good defaults\n",
    "trainer = Trainer()    \n",
    "trainer.fit(model)\n",
    "trainer.test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Still got time ?\n",
    "\n",
    "[Take a glance at the documentation](https://williamfalcon.github.io/pytorch-lightning/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
